{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "2186dc9b-3427-4650-8578-55c497b9d207",
    "_uuid": "688d65e2-3967-4e11-93ae-719ceb5212ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/nlp-getting-started/train.csv\n",
      "/kaggle/input/nlp-getting-started/sample_submission.csv\n",
      "/kaggle/input/nlp-getting-started/test.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import math,random, re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "pd.set_option('display.max_colwidth',1000)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bert-for-tf2\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/b4/1a3da73498960866ad0510ead86b133569ff012bf1c77d82ce95203779fc/bert-for-tf2-0.13.2.tar.gz (40kB)\r\n",
      "\u001b[K     |████████████████████████████████| 40kB 2.3MB/s \r\n",
      "\u001b[?25hCollecting py-params>=0.7.3\r\n",
      "  Downloading https://files.pythonhosted.org/packages/ec/17/71c5f3c0ab511de96059358bcc5e00891a804cd4049021e5fa80540f201a/py-params-0.8.2.tar.gz\r\n",
      "Collecting params-flow>=0.7.1\r\n",
      "  Downloading https://files.pythonhosted.org/packages/0d/12/2604f88932f285a473015a5adabf08496d88dad0f9c1228fab1547ccc9b5/params-flow-0.7.4.tar.gz\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from params-flow>=0.7.1->bert-for-tf2) (1.17.4)\r\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.6/site-packages (from params-flow>=0.7.1->bert-for-tf2) (4.39.0)\r\n",
      "Building wheels for collected packages: bert-for-tf2, py-params, params-flow\r\n",
      "  Building wheel for bert-for-tf2 (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for bert-for-tf2: filename=bert_for_tf2-0.13.2-cp36-none-any.whl size=29938 sha256=268bd8a7bc82bba51355c8c9cf5704ced3a91c94615fbac8f45c200b01e3ed6c\r\n",
      "  Stored in directory: /tmp/.cache/pip/wheels/d8/e1/95/7fa0b466d35f4280a8842a6653f9cd37f89e83832770daf85f\r\n",
      "  Building wheel for py-params (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for py-params: filename=py_params-0.8.2-cp36-none-any.whl size=4633 sha256=143e4ecc685d73125bdf4fc9cc88801c457827d1e1085281ff358cfe1aef2b8e\r\n",
      "  Stored in directory: /tmp/.cache/pip/wheels/83/3a/9c/baf35d6f17f0c2c6b61bf8ac3ab9fc12df0e41432ccaeecacb\r\n",
      "  Building wheel for params-flow (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25h  Created wheel for params-flow: filename=params_flow-0.7.4-cp36-none-any.whl size=16196 sha256=2e83426b1cc8a600d18b806d6d3dad6e43df9ddeca65bd86498db86f0f3f0526\r\n",
      "  Stored in directory: /tmp/.cache/pip/wheels/86/30/40/507b60d68b67ac87f35e95c98f5b296a32f146d5ae1d1d5aa7\r\n",
      "Successfully built bert-for-tf2 py-params params-flow\r\n",
      "Installing collected packages: py-params, params-flow, bert-for-tf2\r\n",
      "Successfully installed bert-for-tf2-0.13.2 params-flow-0.7.4 py-params-0.8.2\r\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.6/site-packages (0.1.83)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install bert-for-tf2\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "import bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "FullTokenizer = bert.bert_tokenization.FullTokenizer\n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\", trainable=False)\n",
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = FullTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cols = [\"id\", \"keyword\", \"location\", \"text\", \"target\"]\n",
    "train = pd.read_csv(\n",
    "    \"/kaggle/input/nlp-getting-started/train.csv\",\n",
    "    header=None,\n",
    "    names=train_cols,\n",
    "    skiprows=1,\n",
    "    engine=\"python\",\n",
    "    encoding=\"latin1\"\n",
    ")\n",
    "\n",
    "test_cols = [\"id\", \"keyword\", \"location\", \"text\"]\n",
    "test = pd.read_csv(\n",
    "    \"/kaggle/input/nlp-getting-started/test.csv\",\n",
    "    header=None,\n",
    "    names=test_cols,\n",
    "    skiprows=1,\n",
    "    engine=\"python\",\n",
    "    encoding=\"latin1\"\n",
    ")\n",
    "\n",
    "def fix_keyword(x):\n",
    "  return str(x[1]).replace('%20', ' ')\n",
    "\n",
    "def new_text(x):\n",
    "  return str(x[1]) + ' ' + str(x[2]) + ' ' +str(x[3])\n",
    "\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "    #tweet = BeautifulSoup(tweet, \"lxml\").get_text()\n",
    "    # Removing the URL links\n",
    "    tweet = re.sub(r\"https?://[A-Za-z0-9./]+\", ' ', tweet)\n",
    "    # Removing the @\n",
    "    tweet = re.sub(r\"@[A-Za-z0-9]+\", ' ', tweet)\n",
    "    # Keeping only letters\n",
    "    tweet = re.sub(r\"[^a-zA-Z0-9!?']\", ' ', tweet)\n",
    "    # Removing additional whitespaces\n",
    "    tweet = re.sub(r\" +\", ' ', tweet)\n",
    "    tweet = re.sub(r\"\\?+\", ' Q', tweet)\n",
    "    tweet = re.sub(r\"\\!+\", ' X', tweet)\n",
    "    return tweet\n",
    "\n",
    "def encode_sentence(sent):\n",
    "    return [\"[CLS]\"] + tokenizer.tokenize(sent) + [\"[SEP]\"]\n",
    "\n",
    "\n",
    "train.keyword = train.apply(fix_keyword, axis=1)  \n",
    "train['new_text'] = train.apply(new_text, axis=1)\n",
    "train_clean = [clean_tweet(tweet) for tweet in train.new_text]\n",
    "train_inputs = [encode_sentence(sentence) for sentence in train_clean]\n",
    "train_labels = train.target.values\n",
    "\n",
    "test.keyword = test.apply(fix_keyword, axis=1)  \n",
    "test['new_text'] = test.apply(new_text, axis=1)\n",
    "test_clean = [clean_tweet(tweet) for tweet in test.new_text]\n",
    "test_inputs = [encode_sentence(sentence) for sentence in test_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ids(tokens):\n",
    "    return tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "def get_mask(tokens):\n",
    "    return np.char.not_equal(tokens, \"[PAD]\").astype(int)\n",
    "\n",
    "def get_segments(tokens):\n",
    "    seg_ids = []\n",
    "    current_seg_id = 0\n",
    "    for tok in tokens:\n",
    "        seg_ids.append(current_seg_id)\n",
    "        if tok == \"[SEP]\":\n",
    "            current_seg_id = 1-current_seg_id # turns 1 into 0 and vice versa\n",
    "    return seg_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_len = [[sent, train_labels[i], len(sent)]\n",
    "                 for i, sent in enumerate(train_inputs)]\n",
    "random.shuffle(data_with_len)\n",
    "data_with_len.sort(key=lambda x: x[2])\n",
    "train_all = [\n",
    "    (\n",
    "        [\n",
    "            get_ids(sent_lab[0]),\n",
    "            get_mask(sent_lab[0]),\n",
    "            get_segments(sent_lab[0])\n",
    "        ],\n",
    "             sent_lab[1]\n",
    "    )\n",
    "    for sent_lab in data_with_len]\n",
    "    #for sent_lab in data_with_len if sent_lab[2] > 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list is a type of iterator so it can be used as generator for a dataset\n",
    "all_dataset = tf.data.Dataset.from_generator(lambda: train_all, output_types=(tf.int32, tf.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "all_batched = all_dataset.padded_batch(BATCH_SIZE,\n",
    "                                       padded_shapes=((3, None), ()),\n",
    "                                       padding_values=(0, 0))\n",
    "\n",
    "NB_BATCHES = math.ceil(len(train_all) / BATCH_SIZE)\n",
    "NB_BATCHES_TEST = NB_BATCHES // 10\n",
    "all_batched.shuffle(NB_BATCHES)\n",
    "test_dataset = all_batched.take(NB_BATCHES_TEST)\n",
    "train_dataset = all_batched.skip(NB_BATCHES_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCNNBERTEmbedding(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 nb_filters=50,\n",
    "                 FFN_units=512,\n",
    "                 nb_classes=2,\n",
    "                 dropout_rate=0.1,\n",
    "                 name=\"dcnn\"):\n",
    "        super(DCNNBERTEmbedding, self).__init__(name=name)\n",
    "        \n",
    "        #self.bert_layer = hub.KerasLayer(\n",
    "        #    \"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
    "        #    trainable=True)\n",
    "\n",
    "        self.bert_layer = hub.KerasLayer(\n",
    "            \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\",\n",
    "            trainable=True)\n",
    "\n",
    "        self.bigram = layers.Conv1D(filters=nb_filters,\n",
    "                                    kernel_size=2,\n",
    "                                    padding=\"valid\",\n",
    "                                    activation=\"relu\")\n",
    "        self.trigram = layers.Conv1D(filters=nb_filters,\n",
    "                                     kernel_size=3,\n",
    "                                     padding=\"valid\",\n",
    "                                     activation=\"relu\")\n",
    "        self.fourgram = layers.Conv1D(filters=nb_filters,\n",
    "                                      kernel_size=4,\n",
    "                                      padding=\"valid\",\n",
    "                                      activation=\"relu\")\n",
    "        self.pool = layers.GlobalMaxPool1D()\n",
    "        self.dense_1 = layers.Dense(units=FFN_units, activation=\"relu\")\n",
    "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
    "        if nb_classes == 2:\n",
    "            self.last_dense = layers.Dense(units=1,\n",
    "                                           activation=\"sigmoid\")\n",
    "        else:\n",
    "            self.last_dense = layers.Dense(units=nb_classes,\n",
    "                                           activation=\"softmax\")\n",
    "    \n",
    "    def embed_with_bert(self, all_tokens):\n",
    "        _, embs = self.bert_layer([all_tokens[:, 0, :],\n",
    "                                   all_tokens[:, 1, :],\n",
    "                                   all_tokens[:, 2, :]])\n",
    "        return embs\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        x = self.embed_with_bert(inputs)\n",
    "\n",
    "        x_1 = self.bigram(x)\n",
    "        x_1 = self.pool(x_1)\n",
    "        x_2 = self.trigram(x)\n",
    "        x_2 = self.pool(x_2)\n",
    "        x_3 = self.fourgram(x)\n",
    "        x_3 = self.pool(x_3)\n",
    "\n",
    "        merged = tf.concat([x_1, x_2, x_3], axis=-1) # (batch_size, 3 * nb_filters)\n",
    "        merged = self.dense_1(merged)\n",
    "        merged = self.dropout(merged, training)\n",
    "        output = self.last_dense(merged)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_FILTERS = 128\n",
    "FFN_UNITS = 256\n",
    "NB_CLASSES = 2\n",
    "\n",
    "DROPOUT_RATE = 0.2\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "NB_EPOCHS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "238/238 [==============================] - 115s 481ms/step - loss: 0.4463 - accuracy: 0.8027\n",
      "Epoch 2/3\n",
      "238/238 [==============================] - 71s 299ms/step - loss: 0.3249 - accuracy: 0.8675\n",
      "Epoch 3/3\n",
      "238/238 [==============================] - 71s 300ms/step - loss: 0.2059 - accuracy: 0.9220\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f105305bd68>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dcnn = DCNNBERTEmbedding(nb_filters=NB_FILTERS,\n",
    "                         FFN_units=FFN_UNITS,\n",
    "                         nb_classes=NB_CLASSES,\n",
    "                         dropout_rate=DROPOUT_RATE)\n",
    "\n",
    "if NB_CLASSES == 2:\n",
    "    Dcnn.compile(loss=\"binary_crossentropy\",\n",
    "                 #optimizer=\"adam\",\n",
    "                 optimizer=tf.optimizers.Adam(learning_rate=2e-5),\n",
    "                 metrics=[\"accuracy\"])\n",
    "else:\n",
    "    Dcnn.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                 optimizer=\"adam\",\n",
    "                 metrics=[\"sparse_categorical_accuracy\"])\n",
    "\n",
    "Dcnn.fit(all_batched, epochs=NB_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     23/Unknown - 6s 262ms/step - loss: 0.1660 - accuracy: 0.9402[0.16597764323587003, 0.9402174]\n"
     ]
    }
   ],
   "source": [
    "results = Dcnn.evaluate(test_dataset)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"id\", \"keyword\", \"location\", \"text\"]\n",
    "test = pd.read_csv(\n",
    "    \"/kaggle/input/nlp-getting-started/test.csv\",\n",
    "    header=None,\n",
    "    names=cols,\n",
    "    skiprows=1,\n",
    "    engine=\"python\",\n",
    "    encoding=\"latin1\"\n",
    ")\n",
    "\n",
    "test.keyword = test.apply(fix_keyword, axis=1)  \n",
    "test['new_text'] = test.apply(new_text, axis=1)  \n",
    "\n",
    "test_clean = [clean_tweet(tweet) for tweet in test.new_text]\n",
    "test_inputs = [encode_sentence(sentence) for sentence in test_clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions made: 100\n",
      "Predictions made: 200\n",
      "Predictions made: 300\n",
      "Predictions made: 400\n",
      "Predictions made: 500\n",
      "Predictions made: 600\n",
      "Predictions made: 700\n",
      "Predictions made: 800\n",
      "Predictions made: 900\n",
      "Predictions made: 1000\n",
      "Predictions made: 1100\n",
      "Predictions made: 1200\n",
      "Predictions made: 1300\n",
      "Predictions made: 1400\n",
      "Predictions made: 1500\n",
      "Predictions made: 1600\n",
      "Predictions made: 1700\n",
      "Predictions made: 1800\n",
      "Predictions made: 1900\n",
      "Predictions made: 2000\n",
      "Predictions made: 2100\n",
      "Predictions made: 2200\n",
      "Predictions made: 2300\n",
      "Predictions made: 2400\n",
      "Predictions made: 2500\n",
      "Predictions made: 2600\n",
      "Predictions made: 2700\n",
      "Predictions made: 2800\n",
      "Predictions made: 2900\n",
      "Predictions made: 3000\n",
      "Predictions made: 3100\n",
      "Predictions made: 3200\n"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "\n",
    "for sentence in test_inputs:\n",
    "    input = [[\n",
    "            get_ids(sentence),\n",
    "            get_mask(sentence),\n",
    "            get_segments(sentence)\n",
    "        ]]\n",
    "    preds.append(int(np.round(Dcnn.predict(input)[0][0])))\n",
    "    \n",
    "    if len(preds) % 100 == 0:\n",
    "        print('Predictions made:', len(preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f0c13446d30>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test['target'] = preds\n",
    "\n",
    "submission = test[['id', 'target']]\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "submission.target.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>10861</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3259</th>\n",
       "      <td>10865</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3260</th>\n",
       "      <td>10868</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>10874</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>10875</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3263 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  target\n",
       "0         0       1\n",
       "1         2       1\n",
       "2         3       1\n",
       "3         9       1\n",
       "4        11       1\n",
       "...     ...     ...\n",
       "3258  10861       1\n",
       "3259  10865       1\n",
       "3260  10868       1\n",
       "3261  10874       1\n",
       "3262  10875       1\n",
       "\n",
       "[3263 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
